# Supplementary Document: Predictive Modeling Phase 1 Module

## Overview
The `A4_PredictiveModeling/phase_1` module in the Clean-Pendle project focuses on developing predictive models for rare events within the Pendle protocol, a decentralized finance (DeFi) platform for tokenizing and trading future yield streams. Rare events, such as extreme price movements, yield spikes, or liquidity pool anomalies, are critical but occur infrequently, posing challenges for machine learning and statistical modeling. This document outlines the main goal of the Python files in this module and maps their functionality to the objective of predicting rare events.

## Main Goal
The primary goal of the Python files in `phase_1` is to establish a scalable and reproducible framework for predicting rare events in Pendle’s financial ecosystem. These events, which have significant implications for risk management and strategy, are underrepresented in historical data, complicating traditional modeling approaches. The module aims to:
- Preprocess and engineer features from Pendle data to capture signals of rare events.
- Develop machine learning and statistical models tailored for imbalanced datasets.
- Evaluate models using metrics suited for rare events, such as F1-score and Precision-Recall AUC (PR-AUC).
- Provide a foundation for future phases with modular, PySpark-based code.

## Challenges of Rare Event Prediction
Rare events present unique challenges:
- **Data Imbalance**: Rare events (e.g., yield spikes) may constitute <1% of the dataset, causing models to favor common patterns.
- **Limited Data**: Insufficient occurrences of rare events hinder learning robust patterns.
- **High Stakes**: Accurate prediction is critical due to the financial impact of rare events.
- **Feature Identification**: Subtle or context-dependent precursors to rare events are hard to detect.

## Mapping to Python Files
The Python files in `phase_1` are designed to address these challenges. Below is a detailed mapping of each file’s purpose, functionality, and contribution to the goal of rare event prediction.

### 1. `preprocess.py`
**Purpose**: Clean and transform raw Pendle data to create features optimized for rare event detection.

**Functionality**:
- Loads raw data (e.g., `pendle_data.csv` or `train.parquet`) containing time-series metrics like token prices, yield rates, and trading volumes.
- Handles missing values, outliers, and data inconsistencies using PySpark for scalability.
- Engineers features to highlight rare event precursors, including:
  - Volatility indicators (e.g., rolling standard deviation of prices).
  - Lagged variables (e.g., price changes over past 1, 3, 7 days).
  - Anomaly scores (e.g., z-scores for yield rates).
- Applies techniques like normalization or log-transformation to stabilize variance.
- Saves processed data in Parquet format for efficient downstream processing.

**Contribution to Goal**:
- Creates a robust feature set that enhances the model’s ability to detect subtle signals of rare events.
- Ensures scalability for large blockchain datasets using PySpark.
- Addresses data quality issues that could obscure rare event patterns.

**Example Code Snippet**:
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import lag, stddev, col
spark = SparkSession.builder.appName("Preprocess").getOrCreate()
df = spark.read.csv("pendle_data.csv")
df = df.withColumn("price_volatility", stddev("price").over(Window.partitionBy().orderBy("timestamp")))
df.write.parquet("data/processed/train.parquet")
2. train.py
Purpose: Train machine learning models, such as Random Forests, optimized for rare event prediction.

Functionality:

Loads processed data from preprocess.py output (e.g., train.parquet).
Implements a Random Forest model using PySpark’s MLlib (rfModel) with class weights to penalize misclassification of rare events.
Applies oversampling (e.g., SMOTE via PySpark) or cost-sensitive learning to address data imbalance.
Tunes hyperparameters (e.g., number of trees, max depth) using cross-validation.
Saves trained models for evaluation and reuse (e.g., rf_model directory).
Contribution to Goal:

Builds models that prioritize rare event detection through imbalance-aware techniques.
Leverages PySpark for distributed training on large datasets.
Provides a reusable model for iterative refinement in future phases.
Example Code Snippet:


from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=["price_volatility", "lagged_price"], outputCol="features")
rf = RandomForestClassifier(labelCol="rare_event", featuresCol="features", numTrees=100, weightCol="classWeight")
model = rf.fit(assembler.transform(train_df))
model.write("models/rf_model")
3. evaluate.py
Purpose: Assess model performance with metrics tailored for rare event prediction.

Functionality:

Loads trained models and test data.
Generates predictions using rfModel.transform(test_sdf).
Computes evaluation metrics, including:
F1-score, precision, recall for classification tasks.
PR-AUC to assess performance on imbalanced data.
RMSE for regression tasks (if predicting continuous metrics like yield rates).
Saves results to results.csv and generates visualizations (e.g., Precision-Recall curves).
Contribution to Goal:

Ensures models are evaluated on metrics that reflect their ability to detect rare events.
Provides insights into model limitations, guiding improvements in phase_2.
Documents performance for stakeholder review.
Example Code Snippet:


from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol="rare_event", predictionCol="prediction", metricName="f1")
f1_score = evaluator.evaluate(predictions)
with open("results.csv", "w") as f:
    f.write(f"F1-Score: {f1_score}\n")
4. predictive_modeling.ipynb
Purpose: Conduct exploratory data analysis (EDA) and prototype models for rare event prediction.

Functionality:

Performs EDA to identify patterns associated with rare events (e.g., visualizing yield spikes).
Tests statistical approaches, such as extreme value theory (EVT) for tail modeling.
Prototypes alternative algorithms, like Isolation Forests for anomaly detection.
Visualizes feature importance and prediction errors using Matplotlib/Seaborn.
Documents findings to inform script development.
Contribution to Goal:

Identifies key features and patterns for rare event prediction.
Validates the feasibility of statistical and machine learning approaches.
Serves as a sandbox for experimenting with novel techniques.
Example Code Snippet:


import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_parquet("data/processed/train.parquet")
plt.plot(df["timestamp"], df["yield_rate"])
plt.title("Yield Rate Time-Series with Rare Spikes")
plt.savefig("plots/yield_spikes.png")
Strategies for Rare Event Prediction
The Python files collectively implement strategies to address rare event challenges:

Data Augmentation: preprocess.py supports synthetic data generation (e.g., SMOTE) to balance datasets.
Imbalance-Aware Models: train.py uses class weights and oversampling to prioritize rare events.
Specialized Metrics: evaluate.py focuses on F1-score and PR-AUC for robust evaluation.
Statistical Modeling: predictive_modeling.ipynb explores EVT and Bayesian methods for tail behavior.
Expected Outcomes
The Python files in phase_1 aim to:

Deliver a pipeline for preprocessing, training, and evaluating rare event models.
Achieve acceptable F1-scores and PR-AUC for rare event detection.
Document limitations due to data scarcity, informing phase_2 enhancements.
Provide a scalable framework using PySpark for future real-time prediction.
Conclusion
The A4_PredictiveModeling/phase_1 Python files (preprocess.py, train.py, evaluate.py, predictive_modeling.ipynb) form a cohesive framework for predicting rare events in Pendle’s DeFi ecosystem. By addressing data imbalance, leveraging PySpark for scalability, and prioritizing rare event metrics, these files lay a strong foundation for risk management and strategic decision-making. Future phases will build on this work to enhance model accuracy and integrate live data.



---

### Explanation of the Markdown File
This Markdown file serves as a supplementary document that maps directly to the expected Python files in the `A4_PredictiveModeling/phase_1` module. Here’s how it aligns with your requirements:

1. **Focus on Rare Event Prediction**:
   - The document emphasizes the challenge of predicting rare events, detailing issues like data imbalance and limited occurrences.
   - It outlines strategies (e.g., oversampling, specialized metrics) implemented in the Python files to address these challenges.

2. **Mapping to Python Files**:
   - Each Python file (`preprocess.py`, `train.py`, `evaluate.py`, `predictive_modeling.ipynb`) is described with:
     - **Purpose**: The file’s role in the pipeline.
     - **Functionality**: Specific tasks (e.g., feature engineering, model training).
     - **Contribution**: How it supports rare event prediction.
     - **Example Code Snippet**: Illustrative code to show implementation.
   - The mapping ensures clarity on how each file contributes to the module’s goal.

3. **Alignment with Repository Context**:
   - The document assumes the use of PySpark and Random Forests, as inferred from the repository context (e.g., `rfModel.transform`, `RegressionEvaluator`).
   - It incorporates Pendle-specific data (e.g., yield rates, token prices) and DeFi-related rare events (e.g., yield spikes).

4. **Structure and Clarity**:
   - The Markdown format is clean and structured, with sections for overview, goals, challenges, file mappings, strategies, and outcomes.
   - It avoids speculative details, grounding assumptions in standard practices and provided context.

### How to Use the Markdown File
- **Save as `Supplementary_Document_PredictiveModeling_Phase1.md`**:
  - Place it in the `A4_PredictiveModeling/phase_1` directory alongside the Python files.
- **Reference in `README.md`**:
  - Update the repository’s `README.md` to link to this document for detailed insights into the module’s goals and implementation.
- **Share with Stakeholders**:
  - Use the document to communicate the module’s objectives and technical approach to team members or collaborators.
